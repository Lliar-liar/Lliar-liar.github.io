<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ziwei Zhou | Fudan University</title>

    <!-- SEO & Social Sharing -->
    <meta name="description"
        content="Personal homepage of Ziwei Zhou, an undergraduate at Fudan University and Research Intern at MSRA, working on Multimodal LLMs and Audio-Visual Generation.">
    <meta property="og:title" content="Ziwei Zhou | Fudan University">
    <meta property="og:description"
        content="Researcher in Multimodal Large Language Models and Audio-Visual Generation.">
    <meta property="og:image" content="profile.jpg">
    <meta property="og:url" content="https://lliar-liar.github.io/">
    <meta property="og:type" content="website">

    <!-- Favicon -->
    <link rel="icon" href="profile.jpg" type="image/jpeg">

    <!-- Styles -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="style.css">
</head>

<body>

    <div class="container">

        <!-- === Left Sidebar === -->
        <aside class="sidebar">
            <!-- Replace 'profile.jpg' with your photo filename -->
            <img src="profile.jpg" alt="Ziwei Zhou" class="profile-img">

            <div class="name-header">Ziwei Zhou</div>
            <div class="affiliation">
                Undergraduate Student<br>
                Fudan University<br>
                <br>
                Research Intern<br>
                Microsoft Research Asia (MSRA)
            </div>

            <div class="contact-info">
                <i class="fas fa-envelope"></i> 22307130208@m.fudan.edu.cn<br>
                <i class="fas fa-map-marker-alt"></i> Shanghai, China
            </div>

            <div class="social-links">
                <!-- Only Github kept as requested -->
                <a href="https://github.com/Lliar-liar" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
                <!-- Add your Google Scholar later if available -->
                <a href="https://scholar.google.com/citations?user=VascmRkAAAAJ&hl=zh-TW" target="_blank"><i
                        class="fas fa-graduation-cap"></i></a>
            </div>
        </aside>

        <!-- === Main Content === -->
        <main class="main-content">

            <!-- Biography -->
            <section id="about">
                <h2>About Me</h2>
                <p>
                    I am a final-year undergraduate student at the <a href="https://cs.fudan.edu.cn/en/">School of
                        Computer Science, Fudan University</a>, supervised by <a
                        href="https://zxwu.azurewebsites.net/">Prof. Zuxuan Wu</a>.
                    Currently, I am also a Research Intern at the Shanghai System and Engineering Group,
                    <strong>Microsoft Research Asia (MSRA)</strong>, working with Senior Research SDE <a
                        href="https://www.microsoft.com/en-us/research/people/yifanyang/">Yifan Yang</a>.
                </p>
                <p>
                    My research interests lie in <strong>Multimodal Large Language Models (MLLMs)</strong>,
                    <strong>Audio-Visual Generation</strong>, and building <strong>Physics-Grounded World
                        Models</strong>.
                    I aim to bridge the gap between multimodal reasoning and high-fidelity generation, enabling AI
                    systems to perceive, reason about, and simulate the physical world with temporal consistency.
                </p>
            </section>

            <!-- News -->
            <section id="news">
                <h2>News</h2>
                <ul class="news-list">
                    <li><span class="date">[Sep. 2025]</span> Submitting "Daily-Omni" to ICLR 2026!</li>
                    <li><span class="date">[Jun. 2025]</span> Joined Microsoft Research Asia (MSRA) as a Research
                        Intern.</li>
                    <li><span class="date">[May. 2025]</span> Released <strong>Daily-Omni</strong> benchmark (arXiv
                        preprint).</li>
                </ul>
            </section>

            <!-- Research / Publications -->
            <section id="publications">
                <h2>Research</h2>

                <!-- 1. Selected Publications (With Image) -->
                <h3>Selected Publications</h3>
                <div class="paper-item">
                    <div class="paper-img">
                        <!-- Replace 'visual_ablation_case.jpg' with your actual file name -->
                        <img src="visual_ablation_case.jpg" alt="Daily-Omni Teaser">
                    </div>
                    <div class="paper-details">
                        <a href="https://arxiv.org/abs/2505.17862" class="paper-title">Daily-Omni: Towards Audio-Visual
                            Reasoning with Temporal Alignment</a>
                        <div class="authors">
                            <strong>Ziwei Zhou</strong>, Rui Wang, Zuxuan Wu
                        </div>
                        <div class="venue">
                            <span class="tag-under-review">Under Review at ICLR 2026</span>
                        </div>
                        <p>
                            We introduce <strong>Daily-Omni</strong>, the first efficiently scalable benchmark for
                            audio-visual question answering.
                            Proposed a novel QA generation pipeline utilizing Gemini 2.0 and DeepSeek-R1.
                            Developed a training-free agent with "Smart Alignment" that achieves SoTA performance among
                            open-source models.
                        </p>
                        <div class="paper-links">
                            <a href="https://arxiv.org/abs/2505.17862">[arXiv]</a>
                            <a href="https://github.com/Lliar-liar/Daily-Omni">[Code]</a>
                            <a href="https://lliar-liar.github.io/Daily-Omni/">[Project Page]</a>
                        </div>
                    </div>
                </div>

                <!-- 2. Ongoing Projects (Text Only) -->
                <h3>Ongoing Research</h3>

                <!-- AR Video Gen -->
                <div class="project-item-text">
                    <span class="paper-title">Unified Autoregressive Model for Synchronized Audio-Visual
                        Generation</span>
                    <div class="authors">
                        <span class="tag-wip">Work in Progress at MSRA</span>
                    </div>
                    <p>
                        Architecting a unified autoregressive model extending Qwen2.5-Omni with discrete audio and video
                        tokens (via FSQ-VAEs).
                        Focusing on synthesizing high-fidelity video with precisely aligned audio, targeting performance
                        comparable to proprietary systems like Google’s Veo3.
                        The model aims to leverage pre-trained reasoning priors to improve generative physical
                        consistency.
                    </p>
                </div>

                <!-- Benchmark -->
                <div class="project-item-text">
                    <span class="paper-title">Benchmarking Framework for Audio-Video Generation</span>
                    <div class="authors">
                        <span class="tag-wip">Work in Progress at MSRA</span>
                    </div>
                    <p>
                        Developing a systematic evaluation benchmark to quantify the quality of generated audio-visual
                        content.
                        Unlike existing metrics that focus on single-modality fidelity, this framework specifically
                        targets <strong>cross-modal synchronization</strong>, <strong>semantic alignment</strong>, and
                        <strong>physical plausibility</strong>.
                        This project addresses a critical gap in standardized evaluation for World Models.
                    </p>
                </div>

            </section>

            <!-- Education -->
            <section id="education">
                <h2>Education</h2>
                <ul>
                    <li>
                        <strong>Fudan University</strong>, Shanghai, China <span style="float:right">Sep. 2022 –
                            Present</span><br>
                        B.S. in Computer Science and Technology<br>

                    </li>
                </ul>
            </section>

        </main>
    </div>

</body>

</html>